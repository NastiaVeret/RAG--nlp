{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever Experiments with Article-Based Chunking & Metadata\n",
    "\n",
    "This notebook demonstrates splitting legal documents by Articles and including metadata (Article Number) in the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\rag-nlp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import docx\n",
    "import re\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from retriever import Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from python-docx) (6.0.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from python-docx) (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chunking with Metadata\n",
    "We extract the Article number and create a structured chunk with `text` and `metadata` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docx_text(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File {file_path} not found\")\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
    "\n",
    "def chunk_by_article_with_metadata(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    pattern = r'Article (\\d+)\\.'\n",
    "    matches = list(re.finditer(pattern, text))\n",
    "    \n",
    "    chunks = []\n",
    "    for i, match in enumerate(matches):\n",
    "        article_num = match.group(1)\n",
    "        start_idx = match.start()\n",
    "        if i + 1 < len(matches):\n",
    "            end_idx = matches[i+1].start()\n",
    "        else:\n",
    "            end_idx = len(text)\n",
    "            \n",
    "        chunk_text = text[start_idx:end_idx].strip()\n",
    "        \n",
    "        if chunk_text:\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": {\n",
    "                    \"article_number\": article_num,\n",
    "                }\n",
    "            })\n",
    "            \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Metadata Chunks ---\n",
      "Generated 158 chunks.\n",
      "Chunk 1 Enriched: {'topics': ['Constitutional Powers', 'Legislative Term', 'Executive Authority'], 'category': 'Governance'}\n",
      "Chunk 2 Enriched: {'topics': ['Legislative Process', 'Constitutional Law', 'Ukrainian Governance'], 'category': 'Governance'}\n",
      "Chunk 3 Enriched: {'topics': ['Human Rights', 'Citizenship', 'Social Protection'], 'category': 'Rights'}\n",
      "Chunk 4 Enriched: {'topics': ['Legislative Initiative', 'Ukrainian Government', 'Lawmaking Process'], 'category': 'Governance'}\n",
      "Chunk 5 Enriched: {'topics': ['Legislative Process', 'Executive Power', 'Constitutional Law'], 'category': 'Governance'}\n",
      "Chunk 6 Enriched: {'topics': ['Budgetary System', 'State Expenditures', 'Public Finance'], 'category': 'Economy'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import getpass\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, provider, model):\n",
    "        self.provider = provider\n",
    "        self.model_name = model\n",
    "        if provider and not model.startswith(f\"{provider}/\"):\n",
    "            self.full_model_name = f\"{provider}/{model}\"\n",
    "        else:\n",
    "            self.full_model_name = model\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        try:\n",
    "            response = completion(\n",
    "                model=self.full_model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "llm = LLM(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "def enrich_with_llm(chunk, llm):\n",
    "    text = chunk.get('text', '')[:1000]\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following legal text.\n",
    "    Return ONLY a JSON object (no markdown) with the following keys:\n",
    "    - \"topics\": A list of 2-3 key topics.\n",
    "    - \"category\": One of [\"Rights\", \"Governance\", \"Judiciary\", \"Economy\", \"General\"].\n",
    "\n",
    "    Text: {text}\n",
    "    \"\"\"\n",
    "    response = llm.generate(prompt)\n",
    "    \n",
    "    if response.startswith(\"Error:\"):\n",
    "        print(f\"LLM Generation Failed: {response}\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        # Clean response\n",
    "        clean = response.replace('```json', '').replace('```', '').strip()\n",
    "        return json.loads(clean)\n",
    "    except Exception as e:\n",
    "        print(f\"JSON Parse Error: {e}. Response was: {response[:100]}...\")\n",
    "        return {}\n",
    "\n",
    "dataset_path = \"dataset-eng.docx\"\n",
    "full_text = load_docx_text(dataset_path)\n",
    "\n",
    "print(\"--- Generating Metadata Chunks ---\")\n",
    "chunks_meta = chunk_by_article_with_metadata(full_text)\n",
    "print(f\"Generated {len(chunks_meta)} chunks.\")\n",
    "\n",
    "for i, chunk in enumerate(chunks_meta[89:95]):\n",
    "    extra_meta = enrich_with_llm(chunk, llm)\n",
    "    if extra_meta:\n",
    "        chunk['metadata'].update(extra_meta)\n",
    "        print(f\"Chunk {i+1} Enriched: {extra_meta}\")\n",
    "    else:\n",
    "        print(f\"Chunk {i+1} Failed to enrich.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunks_to_json(chunks, output_path):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 158 chunks to C:\\Users\\User\\rag-nlp\\chunks_with_metadata6.json\n"
     ]
    }
   ],
   "source": [
    "output_path = \"chunks_with_metadata.json\"\n",
    "save_chunks_to_json(chunks_meta, output_path)\n",
    "\n",
    "print(f\"Saved {len(chunks_meta)} chunks to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieve and Inspect\n",
    "We update the experiment function to correctly handle and display metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_text(chunk):\n",
    "    if isinstance(chunk, dict):\n",
    "        return chunk.get(\"text\", \"\")\n",
    "    return str(chunk)\n",
    "\n",
    "def experiment(retriever_instance, query, method_name, top_k=3):\n",
    "    print(f\"\\nQUERY: {query}\")\n",
    "    print(f\"METHOD: {method_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"--- BM25 ---\")\n",
    "    results = retriever_instance.search_bm25(query, top_k)\n",
    "    for i, r in enumerate(results):\n",
    "        chunk = r['chunk']\n",
    "        text = get_chunk_text(chunk)\n",
    "        meta = chunk.get('metadata', {}) if isinstance(chunk, dict) else {}\n",
    "        \n",
    "        print(f\"[{i+1}] Score: {r['score']:.4f}\")\n",
    "        if meta:\n",
    "            print(f\"Metadata: {meta}\")\n",
    "        print(f\"Content: {text[:150].replace(chr(10), ' ')}...\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    print(f\"\\n--- Semantic ---\")\n",
    "    results = retriever_instance.search_semantic(query, top_k)\n",
    "    for i, r in enumerate(results):\n",
    "        chunk = r['chunk']\n",
    "        text = get_chunk_text(chunk)\n",
    "        meta = chunk.get('metadata', {}) if isinstance(chunk, dict) else {}\n",
    "        \n",
    "        print(f\"[{i+1}] Dist: {r['score']:.4f}\")\n",
    "        if meta:\n",
    "            print(f\"Metadata: {meta}\")\n",
    "        print(f\"Content: {text[:150].replace(chr(10), ' ')}...\")\n",
    "        print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Retriever with Metadata Chunks (English Model)...\n",
      "Loaded 158 chunks from memory.\n",
      "Initializing BM25...\n",
      "Initializing Semantic Search (loading model)...\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:04<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever initialization, complete.\n",
      "\n",
      "QUERY: What is recognised as the highest social value in Ukraine?\n",
      "METHOD: Article Metadata Chunking (English)\n",
      "============================================================\n",
      "--- BM25 ---\n",
      "[1] Score: 21.8988\n",
      "Metadata: {'article_number': '3', 'type': 'article'}\n",
      "Content: Article 3. The human being, his or her life and health, honour and dignity, inviolability and security shall be recognised in Ukraine as the highest s...\n",
      "--------------------\n",
      "[2] Score: 13.8446\n",
      "Metadata: {'article_number': '19', 'type': 'article'}\n",
      "Content: Article 19. The legal order in Ukraine shall be based on the principles according to which no one may be forced to do what is not stipulated by law. G...\n",
      "--------------------\n",
      "[3] Score: 13.1067\n",
      "Metadata: {'article_number': '15', 'type': 'article'}\n",
      "Content: Article 15. Social life in Ukraine shall be based on the principles of political, economic, and ideological diversity. No ideology shall be recognised...\n",
      "--------------------\n",
      "\n",
      "--- Semantic ---\n",
      "[1] Dist: 0.6760\n",
      "Metadata: {'article_number': '3', 'type': 'article'}\n",
      "Content: Article 3. The human being, his or her life and health, honour and dignity, inviolability and security shall be recognised in Ukraine as the highest s...\n",
      "--------------------\n",
      "[2] Dist: 0.8810\n",
      "Metadata: {'article_number': '15', 'type': 'article'}\n",
      "Content: Article 15. Social life in Ukraine shall be based on the principles of political, economic, and ideological diversity. No ideology shall be recognised...\n",
      "--------------------\n",
      "[3] Dist: 0.9418\n",
      "Metadata: {'article_number': '92', 'type': 'article'}\n",
      "Content: Article 92. The following shall be determined exclusively by laws of Ukraine: 1) human and citizens' rights and freedoms, the guarantees of these righ...\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import retriever\n",
    "import importlib\n",
    "importlib.reload(retriever)\n",
    "from retriever import Retriever\n",
    "\n",
    "print(\"Initializing Retriever with Metadata Chunks (English Model)...\")\n",
    "retriever_meta = Retriever(chunks=chunks_meta)\n",
    "\n",
    "query = \"What is recognised as the highest social value in Ukraine?\"\n",
    "experiment(retriever_meta, query, \"Article Metadata Chunking (English)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Retriever with Metadata Chunks (English Model)...\n",
      "Loaded 158 chunks from memory.\n",
      "Initializing BM25...\n",
      "Initializing Semantic Search (loading model)...\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:04<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever initialization, complete.\n",
      "\n",
      "QUERY: What does Ukraine consider the most important social value?\n",
      "METHOD: Article Metadata Chunking (English)\n",
      "============================================================\n",
      "--- BM25 ---\n",
      "[1] Score: 14.2993\n",
      "Metadata: {'article_number': '17', 'type': 'article'}\n",
      "Content: Article 17. Protecting the sovereignty and territorial integrity of Ukraine, ensuring its economic and information security shall be the most importan...\n",
      "--------------------\n",
      "[2] Score: 11.2597\n",
      "Metadata: {'article_number': '3', 'type': 'article'}\n",
      "Content: Article 3. The human being, his or her life and health, honour and dignity, inviolability and security shall be recognised in Ukraine as the highest s...\n",
      "--------------------\n",
      "[3] Score: 9.7513\n",
      "Metadata: {'article_number': '19', 'type': 'article'}\n",
      "Content: Article 19. The legal order in Ukraine shall be based on the principles according to which no one may be forced to do what is not stipulated by law. G...\n",
      "--------------------\n",
      "\n",
      "--- Semantic ---\n",
      "[1] Dist: 0.7006\n",
      "Metadata: {'article_number': '3', 'type': 'article'}\n",
      "Content: Article 3. The human being, his or her life and health, honour and dignity, inviolability and security shall be recognised in Ukraine as the highest s...\n",
      "--------------------\n",
      "[2] Dist: 0.8775\n",
      "Metadata: {'article_number': '15', 'type': 'article'}\n",
      "Content: Article 15. Social life in Ukraine shall be based on the principles of political, economic, and ideological diversity. No ideology shall be recognised...\n",
      "--------------------\n",
      "[3] Dist: 0.9468\n",
      "Metadata: {'article_number': '92', 'type': 'article'}\n",
      "Content: Article 92. The following shall be determined exclusively by laws of Ukraine: 1) human and citizens' rights and freedoms, the guarantees of these righ...\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import retriever\n",
    "import importlib\n",
    "importlib.reload(retriever)\n",
    "from retriever import Retriever\n",
    "\n",
    "print(\"Initializing Retriever with Metadata Chunks (English Model)...\")\n",
    "retriever_meta = Retriever(chunks=chunks_meta)\n",
    "\n",
    "query = \"What does Ukraine consider the most important social value?\"\n",
    "experiment(retriever_meta, query, \"Article Metadata Chunking (English)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"Article 1. Ukraine shall be a sovereign and independent, democratic, social, law-based state. {For official interpretation of Article 1, see Constitutional Court Judgment No. 3-rp/2012 of 25 January 2012}\",\n",
      "    \"metadata\": {\n",
      "      \"article_number\": \"1\",\n",
      "      \"type\": \"article\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Article 2. The sovereignty of Ukraine shall extend throughout its entire territory. Ukraine shall be a unitary state. The territory of Ukraine within its present border shall be indivisible and inviolable.\",\n",
      "    \"metadata\": {\n",
      "      \"article_number\": \"2\",\n",
      "      \"type\": \"article\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(chunks_meta[:2], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Chunks\n",
    "We save the processed chunks with metadata to a JSON file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 158 chunks to chunks.json\n"
     ]
    }
   ],
   "source": [
    "output_file = \"chunks.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_meta, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "print(f\"Saved {len(chunks_meta)} chunks to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration with LLM (Generation)\n",
    "We use **LiteLLM** with a custom wrapper to interact with various providers (Groq, OpenAI, Ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting litellm\n",
      "  Using cached litellm-1.80.9-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting aiohttp>=3.10 (from litellm)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting click (from litellm)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fastuuid>=0.13.0 (from litellm)\n",
      "  Using cached fastuuid-0.14.0-cp312-cp312-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting grpcio<1.68.0,>=1.62.3 (from litellm)\n",
      "  Downloading grpcio-1.67.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from litellm) (0.28.1)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from litellm) (3.1.6)\n",
      "Collecting jsonschema<5.0.0,>=4.22.0 (from litellm)\n",
      "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting openai>=2.8.0 (from litellm)\n",
      "  Using cached openai-2.11.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from litellm) (2.12.5)\n",
      "Collecting python-dotenv>=0.2.0 (from litellm)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken>=0.7.0 (from litellm)\n",
      "  Using cached tiktoken-0.12.0-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from litellm) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.3)\n",
      "Collecting attrs>=22.2.0 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Downloading referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Downloading rpds_py-0.30.0-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.10->litellm)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp>=3.10->litellm)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.10->litellm)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.10->litellm)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp>=3.10->litellm)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp>=3.10->litellm)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-win_amd64.whl.metadata (77 kB)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.10->litellm) (3.11)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=6.8.0->litellm)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=2.8.0->litellm)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai>=2.8.0->litellm)\n",
      "  Using cached jiter-0.12.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting sniffio (from openai>=2.8.0->litellm)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from tqdm>4->openai>=2.8.0->litellm) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from tokenizers->litellm) (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2025.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\rag-nlp\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (6.0.3)\n",
      "Using cached litellm-1.80.9-py3-none-any.whl (11.1 MB)\n",
      "Downloading grpcio-1.67.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 2.1/4.3 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.7/4.3 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 9.7 MB/s  0:00:00\n",
      "Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Downloading aiohttp-3.13.2-cp312-cp312-win_amd64.whl (453 kB)\n",
      "Downloading multidict-6.7.0-cp312-cp312-win_amd64.whl (46 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-win_amd64.whl (87 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached fastuuid-0.14.0-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached openai-2.11.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.12.0-cp312-cp312-win_amd64.whl (205 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.30.0-cp312-cp312-win_amd64.whl (240 kB)\n",
      "Using cached tiktoken-0.12.0-cp312-cp312-win_amd64.whl (878 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zipp, sniffio, rpds-py, python-dotenv, propcache, multidict, jiter, grpcio, frozenlist, fastuuid, distro, click, attrs, aiohappyeyeballs, yarl, tiktoken, referencing, importlib-metadata, aiosignal, openai, jsonschema-specifications, aiohttp, jsonschema, litellm\n",
      "\n",
      "   ----- ----------------------------------  3/24 [python-dotenv]\n",
      "   ----- ----------------------------------  3/24 [python-dotenv]\n",
      "   -------- -------------------------------  5/24 [multidict]\n",
      "   ----------- ----------------------------  7/24 [grpcio]\n",
      "   ----------- ----------------------------  7/24 [grpcio]\n",
      "   ----------- ----------------------------  7/24 [grpcio]\n",
      "   ----------- ----------------------------  7/24 [grpcio]\n",
      "   ----------- ----------------------------  7/24 [grpcio]\n",
      "   ----------- ----------------------------  7/24 [grpcio]\n",
      "   ----------- ----------------------------  7/24 [grpcio]\n",
      "   ----------- ----------------------------  7/24 [grpcio]\n",
      "   ---------------- ----------------------- 10/24 [distro]\n",
      "   ------------------ --------------------- 11/24 [click]\n",
      "   ------------------ --------------------- 11/24 [click]\n",
      "   ------------------ --------------------- 11/24 [click]\n",
      "   -------------------- ------------------- 12/24 [attrs]\n",
      "   -------------------- ------------------- 12/24 [attrs]\n",
      "   ----------------------- ---------------- 14/24 [yarl]\n",
      "   ------------------------- -------------- 15/24 [tiktoken]\n",
      "   ------------------------- -------------- 15/24 [tiktoken]\n",
      "   -------------------------- ------------- 16/24 [referencing]\n",
      "   ---------------------------- ----------- 17/24 [importlib-metadata]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   ------------------------------- -------- 19/24 [openai]\n",
      "   --------------------------------- ------ 20/24 [jsonschema-specifications]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ------------------------------------ --- 22/24 [jsonschema]\n",
      "   ------------------------------------ --- 22/24 [jsonschema]\n",
      "   ------------------------------------ --- 22/24 [jsonschema]\n",
      "   ------------------------------------ --- 22/24 [jsonschema]\n",
      "   ------------------------------------ --- 22/24 [jsonschema]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   -------------------------------------- - 23/24 [litellm]\n",
      "   ---------------------------------------- 24/24 [litellm]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 attrs-25.4.0 click-8.3.1 distro-1.9.0 fastuuid-0.14.0 frozenlist-1.8.0 grpcio-1.67.1 importlib-metadata-8.7.0 jiter-0.12.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 litellm-1.80.9 multidict-6.7.0 openai-2.11.0 propcache-0.4.1 python-dotenv-1.2.1 referencing-0.37.0 rpds-py-0.30.0 sniffio-1.3.1 tiktoken-0.12.0 yarl-1.22.0 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import litellm\n",
    "except ImportError:\n",
    "    !pip install litellm\n",
    "    import litellm\n",
    "\n",
    "from litellm import completion\n",
    "import os\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, provider, model):\n",
    "        self.provider = provider\n",
    "        self.model_name = model\n",
    "        if provider and not model.startswith(f\"{provider}/\"):\n",
    "            self.full_model_name = f\"{provider}/{model}\"\n",
    "        else:\n",
    "            self.full_model_name = model\n",
    "            \n",
    "    def generate(self, prompt):\n",
    "        try:\n",
    "            response = completion(\n",
    "                model=self.full_model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Generation Error with {self.full_model_name}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Groq with model: llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = LLM(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "print(\"Configured Groq with model: llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the highest social value in Ukraine?\n",
      "Retrieved 3 chunks.\n",
      "Generating answer with groq/llama-3.3-70b-versatile...\n",
      "\n",
      "--- Answer ---\n",
      "The human being, his or her life and health, honour and dignity, inviolability and security.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The human being, his or her life and health, honour and dignity, inviolability and security.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rag_pipeline(retriever, query, llm_instance, top_k=3):\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    results = retriever.search_semantic(query, top_k=top_k)\n",
    "    \n",
    "    context_parts = []\n",
    "    for r in results:\n",
    "        chunk = r['chunk']\n",
    "        txt = get_chunk_text(chunk)\n",
    "        if isinstance(chunk, dict) and 'metadata' in chunk:\n",
    "             meta = chunk['metadata']\n",
    "             txt = f\"[Article {meta.get('article_number', '?')}] {txt}\"\n",
    "        context_parts.append(txt)\n",
    "    \n",
    "    full_context = \"\\n\\n\".join(context_parts)\n",
    "    print(f\"Retrieved {len(results)} chunks.\")\n",
    "    \n",
    "    print(f\"Generating answer with {llm_instance.full_model_name}...\")\n",
    "    \n",
    "    prompt = f\"\"\"You are a helpful assistant. Answer the question based ONLY on the following context.\n",
    "    \n",
    "    Context:\n",
    "    {full_context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    answer = llm_instance.generate(prompt)\n",
    "    \n",
    "    print(\"\\n--- Answer ---\")\n",
    "    print(answer)\n",
    "    return answer\n",
    "\n",
    "query = \"What is the highest social value in Ukraine?\"\n",
    "rag_pipeline(retriever_meta, query, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reranking\n",
    "We implement a **Cross-Encoder** to re-score the top retrieved results. This improves precision by considering the query and document interaction more deeply than vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\rag-nlp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEncoder loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "try:\n",
    "    reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512)\n",
    "    print(\"CrossEncoder loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CrossEncoder: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(query, initial_results, top_k=3):\n",
    "    \"\"\"Reranks a list of retrieved results using the CrossEncoder.\"\"\"\n",
    "    if not initial_results:\n",
    "        return []\n",
    "    \n",
    "    pairs = []\n",
    "    for res in initial_results:\n",
    "        chunk = res['chunk']=\n",
    "        text = get_chunk_text(chunk) \n",
    "        pairs.append([query, text])\n",
    "    \n",
    "    scores = reranker_model.predict(pairs)\n",
    "    \n",
    "    reranked_results = []\n",
    "    for i, res in enumerate(initial_results):\n",
    "        new_res = res.copy()\n",
    "        new_res['rerank_score'] = float(scores[i])\n",
    "        reranked_results.append(new_res)\n",
    "        \n",
    "    reranked_results.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "    \n",
    "    return reranked_results[:top_k]\n",
    "\n",
    "def rag_pipeline_with_rerank(retriever, query, llm_instance, retrieve_top_k=10, final_top_k=3):\n",
    "    print(f\"QUERY: {query}\")\n",
    "    \n",
    "    print(f\"1. Retrieving top {retrieve_top_k} candidates...\")\n",
    "    initial = retriever.search_semantic(query, top_k=retrieve_top_k)\n",
    "    \n",
    "    print(f\"2. Reranking...\")\n",
    "    reranked = rerank_results(query, initial, top_k=final_top_k)\n",
    "    \n",
    "    print(\"\\n--- Top Chunks after Reranking ---\")\n",
    "    context_parts = []\n",
    "    for i, r in enumerate(reranked):\n",
    "        chunk = r['chunk']\n",
    "        txt = get_chunk_text(chunk)\n",
    "        score = r.get('rerank_score', 0)\n",
    "        print(f\"[{i+1}] Score: {score:.4f} | {txt[:100]}...\")\n",
    "        \n",
    "        if isinstance(chunk, dict) and 'metadata' in chunk:\n",
    "             meta = chunk['metadata']\n",
    "             txt = f\"[Article {meta.get('article_number', '?')}] {txt}\"\n",
    "        context_parts.append(txt)\n",
    "    \n",
    "    full_context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    print(f\"\\n3. Generating answer with {llm_instance.full_model_name}...\")\n",
    "    prompt = f\"\"\"You are a helpful assistant. Answer the question based ONLY on the following context.\n",
    "    \n",
    "    Context:\n",
    "    {full_context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    answer = llm_instance.generate(prompt)\n",
    "    \n",
    "    print(\"\\n--- Answer ---\")\n",
    "    print(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: What is the highest social value in Ukraine?\n",
      "1. Retrieving top 10 candidates...\n",
      "2. Reranking...\n",
      "\n",
      "--- Top Chunks after Reranking ---\n",
      "[1] Score: 7.4847 | Article 3. The human being, his or her life and health, honour and dignity, inviolability and securi...\n",
      "[2] Score: -0.2736 | Article 15. Social life in Ukraine shall be based on the principles of political, economic, and ideo...\n",
      "[3] Score: -2.3903 | Article 95. The budgetary system of Ukraine shall be based on the principles of fair and impartial d...\n",
      "\n",
      "3. Generating answer with groq/llama-3.3-70b-versatile...\n",
      "\n",
      "--- Answer ---\n",
      "The human being, his or her life and health, honour and dignity, inviolability and security.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The human being, his or her life and health, honour and dignity, inviolability and security.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the highest social value in Ukraine?\"\n",
    "rag_pipeline_with_rerank(retriever_meta, query, llm, retrieve_top_k=10, final_top_k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
