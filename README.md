# RAG--nlp
<img width="521" height="348" alt="image" src="https://github.com/user-attachments/assets/979f8755-749c-4623-9722-f4c487cc6ec3" />

Цей проєкт реалізує систему Retrieval-Augmented Generation (RAG) для інтелектуального пошуку та генерації відповідей на основі Конституції України.
Система дозволяє ставити запитання природною мовою та отримувати точні відповіді з посиланнями на конкретні статті.

Проєкт побудований за класичною RAG-архітектурою:
Семантичний пошук по векторних представленнях тексту
Фільтрація за метаданими (категорія, теми, номер статті)
Переранжування результатів

Генерація відповіді LLM на основі знайдених фрагментів
# Перший крок. Набір даних
В якості набору даних я обрала Конституцію України. Спершу завантажила українську версію, проте з нею були проблеми з точністю. Тому я скачала англійську версію, яка працювала краще

# Другий крок
Далі наступним кроком я розбивала на chunks. Пробувала я різними способами - наприклад по кількості слів, до певного стоп токена. Далі під час тестування я виявила, що якщо наприклад спитати "в якій статті було згадано про...", модель не могла витягти
Текст було розбито по статтях Конституції, тобто одна стаття = один chunk.
Це зберегло юридичний контекст і значно підвищило точність відповідей.

# Третій крок
Для перетворення тексту у векторні представлення використано:
Модель: all-MiniLM-L6-v2
Бібліотека: sentence-transformers

Чому саме ця модель?
дуже швидка;
добре працює навіть на CPU;
є стандартом для семантичного пошуку;
оптимальний баланс між швидкістю та якістю.

# Пошук схожих документів (FAISS)
Далі для того, щоб знаходити схожі елементи серед документів, я використовувала FAISS index
будується індекс по ембедінгах;
виконується швидкий пошук top-k кандидатів;
далі застосовується фільтрація за метаданими.

# Metadata Filtering
Метадані включають:
category — категорія (Rights, Governance, Judiciary, Economy)
topics — теми статті
article_number — номер статті

# Генерація фільтрів за допомогою LLM
Для аналізу наміру користувача використовується Groq LLM
Модель: llama-3.3-70b-versatile

# Переранжування результатів
Модель: cross-encoder/ms-marco-MiniLM-L-6-v2
Cross-Encoder оцінює пари (запит, документ)
Обираються top-3 найбільш релевантні статті

# Генерація відповіді
LLM формує фінальну відповідь:
використовуючи лише знайдені статті;
додаючи посилання на джерела (номери статей).
